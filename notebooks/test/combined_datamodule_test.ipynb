{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12aaa85-6348-4266-8783-811223cd41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from argparse import Namespace\n",
    "from typing import Dict, Optional\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "# script_dir = os.path.dirname(__file__)\n",
    "# parent_dir = os.path.dirname(script_dir)\n",
    "# parent_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "# print(f\"{script_dir = }\")\n",
    "# print(f\"{parent_dir = }\")\n",
    "# sys.path.append(parent_dir)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import lightning as L\n",
    "from lightning import LightningDataModule\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.utilities import CombinedLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d050082-d5d9-4284-ae42-da0fd4d7a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples=1000,\n",
    "        num_classes=5,\n",
    "        image_size=224,\n",
    "        task_type=\"classification\",\n",
    "    ):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_classes = num_classes\n",
    "        self.image_size = image_size\n",
    "        self.task_type = task_type\n",
    "\n",
    "        self.data = torch.randn(num_samples, 3, image_size, image_size)\n",
    "\n",
    "        if task_type == \"classification\":\n",
    "            self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "        elif task_type == \"regression\":\n",
    "            self.labels = torch.randn(num_samples, 1)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid task_type. Choose 'classification' or 'regression'.\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class SyntheticDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples=1000,\n",
    "        num_classes=5,\n",
    "        image_size=224,\n",
    "        batch_size=32,\n",
    "        task_type=\"classification\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.num_classes = num_classes\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.task_type = task_type\n",
    "\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        dataset = SyntheticDataset(\n",
    "            self.num_samples, self.num_classes, self.image_size, self.task_type\n",
    "        )\n",
    "\n",
    "        train_size = int(0.7 * len(dataset))\n",
    "        val_size = int(0.15 * len(dataset))\n",
    "        test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "            dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "class CombinedSyntheticDataModule(LightningDataModule):\n",
    "    def __init__(self, hparams: Namespace):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.datasets = {\n",
    "            \"classification\": SyntheticDataModule(\n",
    "                num_samples=hparams.num_samples,\n",
    "                num_classes=hparams.num_classes,\n",
    "                image_size=hparams.image_size,\n",
    "                batch_size=hparams.batch_size,\n",
    "                task_type=\"classification\",\n",
    "            ),\n",
    "            \"regression\": SyntheticDataModule(\n",
    "                num_samples=hparams.num_samples,\n",
    "                num_classes=1,\n",
    "                image_size=hparams.image_size,\n",
    "                batch_size=hparams.batch_size,\n",
    "                task_type=\"regression\",\n",
    "            ),\n",
    "        }\n",
    "        self.mode = (\n",
    "            self.hparams.mode\n",
    "        )  # 'max_size_cycle' or other modes supported by CombinedLoader\n",
    "\n",
    "    def prepare_data(self):\n",
    "        for dataset in self.datasets.values():\n",
    "            dataset.prepare_data()\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        for dataset in self.datasets.values():\n",
    "            dataset.setup(stage)\n",
    "\n",
    "    def _get_combined_loader(self, loader_method: str) -> CombinedLoader:\n",
    "        loaders = {}\n",
    "        for name, dataset in self.datasets.items():\n",
    "            loader = getattr(dataset, loader_method)()\n",
    "            if loader is not None:  # Some datasets might not have all loader types\n",
    "                loaders[name] = loader\n",
    "        return CombinedLoader(loaders, mode=self.mode)\n",
    "\n",
    "    def train_dataloader(self) -> CombinedLoader:\n",
    "        return self._get_combined_loader(\"train_dataloader\")\n",
    "\n",
    "    def val_dataloader(self) -> CombinedLoader:\n",
    "        return self._get_combined_loader(\"val_dataloader\")\n",
    "\n",
    "    def test_dataloader(self) -> CombinedLoader:\n",
    "        return self._get_combined_loader(\"test_dataloader\")\n",
    "\n",
    "    def predict_dataloader(self) -> CombinedLoader:\n",
    "        return self._get_combined_loader(\n",
    "            \"test_dataloader\"\n",
    "        )  # Using test_dataloader for predict\n",
    "\n",
    "    def get_dataset_info(self) -> Dict[str, Dict[str, int]]:\n",
    "        info = {}\n",
    "        for name, dataset in self.datasets.items():\n",
    "            info[name] = {\n",
    "                \"train\": (\n",
    "                    len(dataset.train_dataloader())\n",
    "                    if hasattr(dataset, \"train_dataloader\")\n",
    "                    else 0\n",
    "                ),\n",
    "                \"val\": (\n",
    "                    len(dataset.val_dataloader())\n",
    "                    if hasattr(dataset, \"val_dataloader\")\n",
    "                    else 0\n",
    "                ),\n",
    "                \"test\": (\n",
    "                    len(dataset.test_dataloader())\n",
    "                    if hasattr(dataset, \"test_dataloader\")\n",
    "                    else 0\n",
    "                ),\n",
    "            }\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f557393-7974-455e-b323-4ad3e2f9709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating combined synthetic data module.\n"
     ]
    }
   ],
   "source": [
    "# Choose the task type\n",
    "task_type = \"combined\"  # Options: \"classification\", \"regression\", or \"combined\"\n",
    "\n",
    "num_classes = 5\n",
    "num_samples = 1000\n",
    "image_size = 224\n",
    "batch_size = 32\n",
    "\n",
    "if task_type == \"combined\":\n",
    "    print(\"Creating combined synthetic data module.\")\n",
    "\n",
    "    # Create the combined synthetic datamodule\n",
    "    hparams = Namespace(\n",
    "        num_samples=num_samples,\n",
    "        num_classes=num_classes,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        mode=\"sequential\",\n",
    "    )\n",
    "    datamodule = CombinedSyntheticDataModule(hparams)\n",
    "else:\n",
    "    # Create the single task synthetic datamodule\n",
    "    if task_type == \"regression\":\n",
    "        num_classes = 1\n",
    "    datamodule = SyntheticDataModule(\n",
    "        num_samples=num_samples,\n",
    "        num_classes=num_classes,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        task_type=task_type,\n",
    "    )\n",
    "\n",
    "# Prepare data and setup\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f32d661-42f4-44c8-b29c-44e979f101ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bff02895-8c0d-478c-8214-7d9dec910a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58627df7-b92b-4bfe-8ac2-9cdd2f48e4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86ee9ac4-257a-41fa-bbf3-bab48d7fc145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37f80933-be2a-48b6-95d2-85cb3d8bea19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
